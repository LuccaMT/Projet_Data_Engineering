"""
Script pour r√©cup√©rer l'historique complet des matchs sur plusieurs mois
afin de maximiser le nombre de ligues dans la base de donn√©es.

Usage:
    python fetch_historical.py --months 6
    python fetch_historical.py --start-date 2024-06-01 --end-date 2024-12-17
"""

import argparse
import os
from dataclasses import asdict
from datetime import date, timedelta
from typing import Tuple

import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

import sys
sys.path.insert(0, os.path.dirname(__file__))

from flashscore_feed import (
    FEED_URL,
    REQUEST_HEADERS,
    Match,
    _date_to_offset,
    daterange,
    parse_feed,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="R√©cup√®re l'historique des matchs sur plusieurs mois pour maximiser les ligues.",
    )
    parser.add_argument(
        "--months",
        type=int,
        default=6,
        help="Nombre de mois d'historique √† r√©cup√©rer (par d√©faut: 6 mois).",
    )
    parser.add_argument(
        "--start-date",
        help="Date de d√©but au format YYYY-MM-DD (optionnel, sinon calcul√© depuis --months).",
    )
    parser.add_argument(
        "--end-date",
        help="Date de fin au format YYYY-MM-DD (par d√©faut: aujourd'hui).",
    )
    parser.add_argument(
        "--variant",
        type=int,
        default=0,
        help="Variante du flux Flashscore (laisser 0 sauf besoin sp√©cifique).",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Afficher les dates qui seront scrap√©es sans lancer le scraping.",
    )
    return parser.parse_args()


def resolve_date_range(args: argparse.Namespace) -> Tuple[date, date]:
    """D√©termine la plage de dates √† scraper."""
    end = date.fromisoformat(args.end_date) if args.end_date else date.today()
    
    if args.start_date:
        start = date.fromisoformat(args.start_date)
    else:
        # Calculer depuis N mois en arri√®re
        months = max(1, min(args.months, 12))  # Limiter √† 12 mois
        # Approximation: 1 mois = 30 jours
        start = end - timedelta(days=months * 30)
    
    return start, end


class HistoricalSpider(scrapy.Spider):
    name = "flashscore_historical"
    
    def __init__(self, target_date: date, variant: int, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.target_date = target_date
        self.variant = variant
    
    def start_requests(self):
        offset = _date_to_offset(self.target_date)
        
        # V√©rifier si la date est dans la limite de Flashscore (¬±7 jours)
        if abs(offset) > 7:
            self.logger.warning(
                f"Date {self.target_date} hors limite Flashscore (offset={offset}). "
                f"Flashscore limite √† ¬±7 jours. Les donn√©es peuvent √™tre incompl√®tes."
            )
        
        url = FEED_URL.format(sport_id=1, offset=offset, variant=self.variant)
        yield scrapy.Request(
            url,
            headers=REQUEST_HEADERS,
            callback=self.parse_feed_response,
            dont_filter=True,
            meta={"target_date": self.target_date},
            errback=self.handle_error,
        )
    
    def parse_feed_response(self, response: scrapy.http.TextResponse):
        """Parse le flux et extrait tous les matchs (termin√©s et √† venir)."""
        match_count = 0
        for match in parse_feed(response.text):
            item = asdict(match)
            item["target_date"] = response.meta["target_date"].isoformat()
            
            # Router vers la bonne collection selon le statut
            if match.status_code == "3":  # Termin√©
                item["collection"] = "matches_finished"
            else:  # √Ä venir ou en cours
                item["collection"] = "matches_upcoming"
            
            match_count += 1
            yield item
        
        self.logger.info(
            f"Date {response.meta['target_date']}: {match_count} matchs r√©cup√©r√©s"
        )
    
    def handle_error(self, failure):
        self.logger.error(f"Erreur de requ√™te: {failure.value}")


def main() -> None:
    args = parse_args()
    start_date, end_date = resolve_date_range(args)
    
    # G√©n√©rer la liste des dates √† scraper
    dates = list(daterange(start_date, end_date))
    
    print("=" * 70)
    print("üìÖ R√âCUP√âRATION DE L'HISTORIQUE DES MATCHS")
    print("=" * 70)
    print(f"Date de d√©but : {start_date}")
    print(f"Date de fin   : {end_date}")
    print(f"Nombre de jours : {len(dates)} jours")
    print(f"P√©riode : {(end_date - start_date).days} jours")
    print()
    
    # Avertissement sur les limitations Flashscore
    today = date.today()
    days_outside = sum(1 for d in dates if abs((d - today).days) > 7)
    if days_outside > 0:
        print("‚ö†Ô∏è  AVERTISSEMENT:")
        print(f"   {days_outside}/{len(dates)} dates sont hors de la fen√™tre Flashscore (¬±7 jours)")
        print("   Ces dates peuvent retourner des donn√©es incompl√®tes ou vides.")
        print("   Pour un historique complet, il faut scraper r√©guli√®rement au fil du temps.")
        print()
    
    if args.dry_run:
        print("üîç Mode dry-run - Dates qui seront scrap√©es:")
        for i, d in enumerate(dates[:10], 1):
            offset = (d - today).days
            print(f"   {i}. {d} (offset: {offset:+d} jours)")
        if len(dates) > 10:
            print(f"   ... et {len(dates) - 10} dates suppl√©mentaires")
        print()
        print("Pour lancer le scraping, r√©ex√©cutez sans --dry-run")
        return
    
    # Configuration MongoDB
    mongo_uri = os.getenv("MONGO_URI", "mongodb://admin:admin123@mongodb:27017/")
    mongo_db = os.getenv("MONGO_DB", "flashscore")
    
    print(f"üìä Configuration MongoDB:")
    print(f"   URI: {mongo_uri}")
    print(f"   Database: {mongo_db}")
    print()
    
    # Charger les settings du projet
    settings = get_project_settings()
    
    # V√©rifier que les pipelines MongoDB sont activ√©s
    if "pipelines.MongoDBPipeline" in str(settings.get("ITEM_PIPELINES", {})):
        print("‚úÖ Pipeline MongoDB activ√©")
    else:
        print("‚ö†Ô∏è  Pipeline MongoDB non trouv√© dans settings.py")
    
    print()
    print("üöÄ Lancement du scraping...")
    print(f"   Cela peut prendre plusieurs minutes pour {len(dates)} jours")
    print()
    
    # Cr√©er et lancer le crawler
    process = CrawlerProcess(settings)
    
    # Lancer un spider pour chaque date
    for target_date in dates:
        process.crawl(
            HistoricalSpider,
            target_date=target_date,
            variant=args.variant,
        )
    
    process.start()
    
    print()
    print("=" * 70)
    print("‚úÖ SCRAPING TERMIN√â")
    print("=" * 70)
    print()
    print("üìä V√©rifier les statistiques dans MongoDB:")
    print("   docker exec flashscore-mongodb mongosh \\")
    print('     "mongodb://admin:admin123@localhost:27017/flashscore?authSource=admin" \\')
    print('     --quiet --eval "')
    print("       print('Matchs √† venir:', db.matches_upcoming.countDocuments({}));")
    print("       print('Matchs termin√©s:', db.matches_finished.countDocuments({}));")
    print("       print('Ligues uniques upcoming:', db.matches_upcoming.distinct('league').length);")
    print("       print('Ligues uniques finished:', db.matches_finished.distinct('league').length);")
    print('     "')
    print()


if __name__ == "__main__":
    main()
